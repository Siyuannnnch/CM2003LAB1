{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Input, Activation, Dropout, BatchNormalization, Conv2DTranspose, Concatenate \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-net construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_layer, n_base, batchnorm = False):\n",
    "    output_layer = Conv2D(filters= n_base, kernel_size=(3,3), strides=(1,1), padding='same')(input_layer)\n",
    "    if batchnorm :\n",
    "        output_layer = BatchNormalization()(output_layer)\n",
    "    output_layer = Activation('relu')(output_layer)\n",
    "    output_layer = Conv2D(filters=n_base, kernel_size=(3,3), strides=(1,1), padding='same')(output_layer)\n",
    "    if batchnorm :\n",
    "        output_layer = BatchNormalization()(output_layer)\n",
    "    output_layer = Activation('relu')(output_layer)\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(input_layer,n_base , batchnorm = False, dropout = False):\n",
    "    out = conv_block(input_layer, n_base, batchnorm = batchnorm)\n",
    "    out2 = MaxPooling2D(pool_size=(2,2))(out)\n",
    "    if dropout:\n",
    "        out2 = Dropout(0.2)(out2)\n",
    "    return out, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(input_layer, layer2conc, n_base, batchnorm = False, dropout = False):\n",
    "    print(str(n_base), input_layer.shape)\n",
    "    output_layer = Conv2DTranspose(filters = n_base,  kernel_size=(3,3), strides=(2, 2), padding=\"same\")(input_layer)\n",
    "    print(str(n_base), output_layer.shape)\n",
    "    output_layer = Concatenate()([output_layer, layer2conc])\n",
    "    if dropout:\n",
    "        output_layer = Dropout(0.2)(output_layer)\n",
    "    output_layer = conv_block(output_layer, n_base, batchnorm = batchnorm)\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(img_ch, img_width, img_height, n_base, dropout = False, batchnormal = False, binary = True, class_num = 2):\n",
    "    input_layer = Input(shape=(img_width, img_height, img_ch))\n",
    "    \n",
    "    #Encoder\n",
    "    e1, em1 = encoder_block(input_layer, n_base, batchnorm = batchnormal, dropout = dropout)\n",
    "    e2, em2 = encoder_block(em1, n_base*2, batchnorm = batchnormal, dropout = dropout)\n",
    "    e3, em3 = encoder_block(em2, n_base*4, batchnorm = batchnormal, dropout = dropout)\n",
    "    e4, em4 = encoder_block(em3, n_base*8, batchnorm = batchnormal, dropout = dropout)\n",
    "\n",
    "    #Bottleneck \n",
    "    bottleneck = conv_block(em4, n_base*16, batchnorm = batchnormal)\n",
    "\n",
    "    #Decoder\n",
    "    d_block1 = decoder_block(bottleneck, e4, n_base*8, batchnorm = batchnormal, dropout = dropout)\n",
    "    d_block2 = decoder_block(d_block1, e3, n_base*4, batchnorm = batchnormal, dropout = dropout)\n",
    "    d_block3 = decoder_block(d_block2, e2, n_base*2, batchnorm = batchnormal, dropout = dropout)\n",
    "    d_block4 = decoder_block(d_block3, e1, n_base, batchnorm = batchnormal, dropout = dropout)\n",
    "    \n",
    "    #Output\n",
    "    if binary:\n",
    "        out = Conv2D(filters=1, kernel_size=(3,3), strides=(1,1), padding='same', activation = 'sigmoid')(d_block4)\n",
    "    else:\n",
    "        out = Conv2D(filters=class_num, kernel_size=(3,3), strides=(1,1), padding='same', activation = 'sigmoid')(d_block4)\n",
    "        \n",
    "    clf = Model(inputs=input_layer, outputs=out)\n",
    "    clf.summary()\n",
    "    \n",
    "    return clf   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 (None, 16, 16, 256)\n",
      "128 (None, 32, 32, 128)\n",
      "64 (None, 32, 32, 128)\n",
      "64 (None, 64, 64, 64)\n",
      "32 (None, 64, 64, 64)\n",
      "32 (None, 128, 128, 32)\n",
      "16 (None, 128, 128, 32)\n",
      "16 (None, 256, 256, 16)\n",
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 256, 256, 16) 160         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 256, 256, 16) 64          conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 256, 256, 16) 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 256, 256, 16) 2320        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 256, 256, 16) 64          conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 256, 256, 16) 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 128, 128, 16) 0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 128, 128, 16) 0           max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 128, 128, 32) 4640        dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 128, 128, 32) 128         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 128, 128, 32) 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 128, 128, 32) 9248        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 128, 128, 32) 128         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 128, 128, 32) 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 64, 64, 32)   0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 64, 64, 32)   0           max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 64, 64, 64)   18496       dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 64, 64, 64)   256         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 64, 64, 64)   0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 64, 64, 64)   36928       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 64, 64, 64)   256         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 64, 64, 64)   0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 32, 32, 64)   0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 32, 32, 64)   0           max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 32, 32, 128)  73856       dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 32, 32, 128)  512         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 32, 32, 128)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 32, 32, 128)  147584      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 32, 32, 128)  512         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 32, 32, 128)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 16, 16, 128)  0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 16, 16, 128)  0           max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 16, 16, 256)  295168      dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 16, 16, 256)  1024        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 16, 16, 256)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 16, 256)  590080      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 16, 16, 256)  1024        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 16, 16, 256)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_12 (Conv2DTran (None, 32, 32, 128)  295040      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 32, 256)  0           conv2d_transpose_12[0][0]        \n",
      "                                                                 activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 32, 32, 256)  0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 32, 32, 128)  295040      dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 32, 32, 128)  512         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 32, 32, 128)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 32, 32, 128)  147584      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 32, 32, 128)  512         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 32, 32, 128)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_13 (Conv2DTran (None, 64, 64, 64)   73792       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 64, 64, 128)  0           conv2d_transpose_13[0][0]        \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 64, 64, 128)  0           concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 64, 64, 64)   73792       dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 64, 64, 64)   256         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 64, 64, 64)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 64, 64, 64)   36928       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 64, 64, 64)   256         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 64, 64, 64)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_14 (Conv2DTran (None, 128, 128, 32) 18464       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 128, 128, 64) 0           conv2d_transpose_14[0][0]        \n",
      "                                                                 activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 128, 128, 64) 0           concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 128, 128, 32) 18464       dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 128, 128, 32) 128         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 128, 128, 32) 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 128, 128, 32) 9248        activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 128, 128, 32) 128         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 128, 128, 32) 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_15 (Conv2DTran (None, 256, 256, 16) 4624        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 256, 256, 32) 0           conv2d_transpose_15[0][0]        \n",
      "                                                                 activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 256, 256, 32) 0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 256, 256, 16) 4624        dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 256, 256, 16) 64          conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 256, 256, 16) 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 256, 256, 16) 2320        activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 256, 256, 16) 64          conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 256, 256, 16) 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 256, 256, 1)  145         activation_71[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,164,433\n",
      "Trainable params: 2,161,489\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clf = get_unet(1, 256, 256, 16, dropout = True, batchnormal = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 0.0001) / (K.sum(y_true_f) + K.sum(y_pred_f) + 0.0001)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, img_list, mask_list, state):\n",
    "    #Checking that img_list and mask_list are in the same order\n",
    "    mask_list2 = [mask.replace('_Tumor.png', '.png') for mask in mask_list]\n",
    "    if mask_list2 == img_list:\n",
    "        del mask_list2\n",
    "        pass\n",
    "    else :\n",
    "        print('Images and masks are not in the same order')\n",
    "    \n",
    "    #Initialising the final arrays\n",
    "    img_array = np.zeros((len(img_list), img_h, img_w), dtype = np.float32)\n",
    "    mask_array = np.zeros((len(mask_list), img_h, img_w), dtype = np.float32)\n",
    "    \n",
    "    ind = 0\n",
    "    for img_label, mask_label in zip(img_list, mask_list):\n",
    "        #Loading image\n",
    "        img = imread(data_path + 'Image/' + img_label, as_gray = True)\n",
    "        img = resize(img, (img_h, img_w), anti_aliasing = True).astype('float32')\n",
    "        img_array[ind] = (np.array(img)-np.min(np.array(img)))/(np.max(np.array(img))-np.min(np.array(img)))\n",
    "        #Loading mask\n",
    "        img = imread(data_path + 'Mask/' + mask_label, as_gray = True)\n",
    "        img = resize(img, (img_h, img_w), anti_aliasing = True).astype('float32')\n",
    "        mask_array[ind] = (np.array(img)-np.min(np.array(img)))/(np.max(np.array(img))-np.min(np.array(img)))\n",
    "        #Update counter\n",
    "        ind = ind + 1\n",
    "        print(state + ': ' + str(ind) + '/' + str(len(img_list)))\n",
    "    img_array = np.expand_dims(img_array, axis =3)\n",
    "    mask_array = np.expand_dims(mask_array, axis =3)\n",
    "    return img_array, mask_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_h, img_w = 240, 240\n",
    "path = '/DL_course_data/Lab3/MRI/'\n",
    "img_list = os.listdir(path + 'Image/')\n",
    "shuffle(img_list)\n",
    "mask_list = [file.replace('.png', '_Tumor.png') for file in img_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_fold(x_list, y_list, path, K, current_index):\n",
    "    n = len(x_list)\n",
    "    x_test, y_test = load_data(path, \n",
    "                               x_list[int(current_index*n/K):int((current_index+1)*n/K)], \n",
    "                               y_list[int(current_index*n/K):int((current_index+1)*n/K)],\n",
    "                               'Test')\n",
    "    x_train, y_train = load_data(path, \n",
    "                                 x_list[:int(current_index*n/K)] + x_list[int((current_index+1)*n/K):], \n",
    "                                 y_list[:int(current_index*n/K)] + y_list[int((current_index+1)*n/K):],\n",
    "                                 'Train')\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ch, img_h, img_w = 1, 240, 240\n",
    "n_base = 8\n",
    "batchsize = 8\n",
    "LR = 1e-4\n",
    "n_epochs = 100\n",
    "K1 = 3\n",
    "for fold in range(K1):\n",
    "    #Creating the fold\n",
    "    x_train, x_test, y_train, y_test = K_fold(img_list, mask_list, path, K1, fold)\n",
    "    \n",
    "    #Training the network\n",
    "    clf = get_unet(img_ch, img_w, img_h, n_base, dropout = True, batchnormal = True)\n",
    "    clf.compile(loss=[dice_coef_loss], optimizer = Adam(lr = LR), metrics=[dice_coef, Precision(), Recall()]) \n",
    "    clf_hist = clf.fit(x_train, y_train, epochs = n_epochs, batch_size = batchsize, validation_data=(x_test, y_test))\n",
    "    \n",
    "    #Free memory\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del x_test\n",
    "    del y_test\n",
    "    \n",
    "    #Saving results\n",
    "    #Loss\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.plot(clf_hist.history[\"loss\"], label=\"loss\")\n",
    "    plt.plot(clf_hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "    xmin = np.argmin(clf_hist.history[\"val_loss\"])\n",
    "    ymin = np.min(clf_hist.history[\"val_loss\"])\n",
    "    plt.plot( xmin, ymin, marker=\"x\", color=\"r\", label=\"best model\")\n",
    "    plt.annotate('(' + str(xmin) + ', '+ str(round(ymin, 2)) + ')', xy = (xmin, ymin - 0.01),\n",
    "                 horizontalalignment = \"center\", verticalalignment = \"top\", color = \"red\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss Value\")\n",
    "    plt.legend();\n",
    "    plt.savefig('1/loss_fold-'+ str(fold+1) +'.png', dpi = 200)\n",
    "\n",
    "    #Accuracy\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.plot(clf_hist.history[\"dice_coef\"], label=\"accuracy\")\n",
    "    plt.plot(clf_hist.history[\"val_dice_coef\"], label=\"val_accuracy\")\n",
    "    xmax = np.argmax(clf_hist.history[\"val_dice_coef\"])\n",
    "    ymax = np.max(clf_hist.history[\"val_dice_coef\"])\n",
    "    plt.plot( xmax, ymax, marker=\"x\", color=\"r\", label=\"best model\")\n",
    "    plt.annotate('(' + str(xmax) + ', '+ str(round(ymax,2)) + ')', xy = (xmax, ymax + 0.01),\n",
    "                 horizontalalignment = \"center\", verticalalignment = \"bottom\", color = \"red\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy Value\")\n",
    "    plt.legend();\n",
    "    plt.savefig('1/accuracy_fold-' + str(fold+1)+ '.png', dpi = 200)\n",
    "    \n",
    "    #Precision\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.title(\"Precision\")\n",
    "    plt.plot(clf_hist.history['precision'], label=\"Precision\")\n",
    "    plt.plot(clf_hist.history[\"val_precision\"], label=\"val_precision\")\n",
    "    xmax = np.argmax(clf_hist.history[\"val_precision\"])\n",
    "    ymax = np.max(clf_hist.history[\"val_precision\"])\n",
    "    plt.plot( xmax, ymax, marker=\"x\", color=\"r\", label=\"best model\")\n",
    "    plt.annotate('(' + str(xmax) + ', '+ str(round(ymax,2)) + ')', xy = (xmax, ymax + 0.01),\n",
    "                 horizontalalignment = \"center\", verticalalignment = \"bottom\", color = \"red\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy Value\")\n",
    "    plt.legend();\n",
    "    plt.savefig('1/precision_fold-' + str(fold+1)+ '.png', dpi = 200)\n",
    "    \n",
    "    # Recall\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.title(\"Recall\")\n",
    "    plt.plot(clf_hist.history['recall'], label=\"Recall\")\n",
    "    plt.plot(clf_hist.history['val_recall'], label=\"val_Recall\")\n",
    "    xmax = np.argmax(clf_hist.history['val_recall'])\n",
    "    ymax = np.max(clf_hist.history['val_recall'])\n",
    "    plt.plot( xmax, ymax, marker=\"x\", color=\"r\", label=\"best model\")\n",
    "    plt.annotate('(' + str(xmax) + ', '+ str(round(ymax,2)) + ')', xy = (xmax, ymax + 0.01),\n",
    "                 horizontalalignment = \"center\", verticalalignment = \"bottom\", color = \"red\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy Value\")\n",
    "    plt.legend();\n",
    "    plt.savefig('1/recall_fold-' + str(fold+1)+ '.png', dpi = 200)\n",
    "    \n",
    "    del clf\n",
    "    clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
